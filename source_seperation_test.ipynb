{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673c7105",
   "metadata": {},
   "source": [
    "# ðŸŽ¹ðŸŽ»ðŸ¥\n",
    "# Modelling interaction in the jazz rhythm section using source-seperated commercial audio recordings: proof of concept\n",
    "*Huw Cheston* (PhD Candidate @ Centre for Music & Science, University of Cambridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b65173",
   "metadata": {},
   "source": [
    "![Bill Evans trio](https://raw.githubusercontent.com/HuwCheston/source-seperation-test/main/assets/be_trio.jpg \"Bill Evans trio\")\n",
    "*The Bill Evans trio, photographed at the Village Vanguard in 1961. L-R: Scott LaFaro, bass; Bill Evans, piano; Paul Motian, drums.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b780462",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "0. Import-dependencies and set constants\n",
    "1. Load in the stereo audio file and separate it into individual tracks for the piano, bass, and drums performance.\n",
    "    - Here, we use a 30-second excerpt from \"Peri's Scope\" by the Bill Evans Trio, from the 1959 album Portrait in Jazz. The excerpt comes from the middle of Bill Evans' piano solo.\n",
    "2. Generate a tempo map from the complete stereo audio file\n",
    "    - This map corresponds to the estimated position of each crotchet onset\n",
    "3. Detect individual onsets in the isolated tracks\n",
    "    - This is equivalent to detecting the position of *every single note* by a performer\n",
    "4. Coerce the data into the correct format for modelling\n",
    "    - This involves matching the estimated position of each crotchet with the nearest onset by each performer\n",
    "5. Generate the phase correction model and extract the coupling coefficients\n",
    "6. Visualise the degree of coupling between the musicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350fffb",
   "metadata": {},
   "source": [
    "## 0. Import dependencies\n",
    "\n",
    "The following few cells load in our required source separation, signal processing, and data analysis libraries, and set a few constant variables that we refer to throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c134b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spleeter.separator import Separator\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406af0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d3a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio attributes\n",
    "SAMPLE_RATE = 44100\n",
    "HOP_LENGTH = 512\n",
    "INPUT_AUDIO = r\".\\input_.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a74645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spleeter model - vocals/piano/bass/drums/other\n",
    "MODEL = \"spleeter:5stems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92dddab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track details\n",
    "MEAN_TEMPO = 180\n",
    "MIN_TEMPO, MAX_TEMPO = 175, 185\n",
    "SD_TEMPO = 0.1\n",
    "TRACK_TIGHTNESS = 2000\n",
    "DETECTION_THRESHOLD = 1/12    # A triplet quaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413b020",
   "metadata": {},
   "source": [
    "## 1. Source seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a90b67",
   "metadata": {},
   "source": [
    "### Load the audio and define the Spleeter model\n",
    "[*Spleeter*](https://github.com/deezer/spleeter) is a source-separation Python library created by the streaming service Deezer. It is currently used in both [iZotope RX](https://www.izotope.com/en/products/rx.html) and [Steinberg Spectralayers](https://www.steinberg.net/spectralayers/). Currently, it is placed 16th out of 25 entries on the [MUSDB18 Source Separation leaderboard.](https://paperswithcode.com/sota/music-source-separation-on-musdb18)\n",
    "\n",
    "Spleeter contains a variety of pre-trained machine learning models, capable of extracting various combinations of instruments. Here, we use the [5stems model](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-5stems-model) to extract piano, bass, and drums from the stereo recording, discarding the vocals and other (residual) tracks. By default, Spleeter also only performs separation up to 11kHz; however, with [some modification to the internal configuration files](https://github.com/deezer/spleeter/wiki/5.-FAQ#why-are-there-no-high-frequencies-in-the-generated-output-files-) (not ideal for running online!) it can be used up to 22kHz. For now, we just use the defaults, with the caveat that we may increase the sampling rate in the full implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d955bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\input_.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0m\u001b[0;32m   1184\u001b[0m                      \"Error opening {0!r}: \".format(self.name))\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[1;34m(err, prefix)\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error opening '.\\\\input_.mp3': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17172/613802858.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# First, we load the full mix of the audio file in to Librosa.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m full_mix, _ = librosa.load(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mINPUT_AUDIO\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# The filepath to our audio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# Our sample rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmono\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# Load the file in stereo, as we need both channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\audioread\\__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huwch\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\audioread\\rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \"\"\"\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\input_.mp3'"
     ]
    }
   ],
   "source": [
    "# First, we load the full mix of the audio file in to Librosa.\n",
    "full_mix, _ = librosa.load(\n",
    "    INPUT_AUDIO,    # The filepath to our audio\n",
    "    sr=SAMPLE_RATE,    # Our sample rate\n",
    "    mono=False,    # Load the file in stereo, as we need both channels\n",
    "    offset=0,    # Load the file from the very beginning\n",
    "    duration=None,    # Don't trim the audio file  \n",
    "    dtype=np.float32    # Set the data type as a float for use in numpy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1549dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we instantiate our Spleeter Separator instance.\n",
    "# We use the model we specified as a constant earlier (i.e. 5stems)\n",
    "separator = Separator(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd835f81",
   "metadata": {},
   "source": [
    "### Seperate the audio\n",
    "One of the advantages of studying piano trio recordings over larger ensembles is that recording engineers typically used the stereo spectrum in a fairly predictable way. This knowledge of how instruments are panned can help us in source separation. For instance, in this recording, Scott LaFaro's bass is panned over to the right channel, Paul Motian's drums are on the left channel, and Bill Evans' piano is in the centre. We therefore can apply source separation individually to these channels to get the cleanest signal for each instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e185d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_channel(\n",
    "    waveform: np.array, channel: int\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    This function extracts the left or right channel from a stereo audio source.\n",
    "    \"\"\"\n",
    "    return np.tile(waveform[:, channel].T.reshape(-1, 1), (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93463958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an empty dictionary to store our audio in\n",
    "prediction = {}\n",
    "# We apply source separation to the left channel only, for the drums\n",
    "prediction['drums'] = separator.separate(extract_channel(full_mix.T, 0))['drums']\n",
    "# ... and on the right channel for bass\n",
    "prediction['bass'] = separator.separate(extract_channel(full_mix.T, 1))['bass']\n",
    "# We use both channels for keys (these are centre panned)\n",
    "prediction['piano'] = separator.separate(full_mix.T)['piano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's listen to our drums extracted audio to make sure it sounds reasonable\n",
    "Audio(prediction['drums'].T, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f212b",
   "metadata": {},
   "source": [
    "That sounds pretty good to me for now; while we occasionally get a bit of leaking from both the piano (0:12-0:15) and the bass (0:19-0:20), the ride cymbal, snare drum, and bass drum are all cutting through cleanly in the isolated drum track. We'll refine the source separation further in the full implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1689127",
   "metadata": {},
   "source": [
    "## 2. Beat tracking from the full mix ðŸŽ¹ðŸŽ»ðŸ¥\n",
    "\n",
    "To get a general idea of where each beat is in the complete ensemble recording, we now automatically generate a tempo map of the performance using beat tracking. We'll use this to match up the onsets detected in the individual piano, bass, and drum tracks with each crotchet beat in the performance.\n",
    "\n",
    "Librosa implements two algorithms for beat tracking: the beat.beat_track function uses dynamic programming and peak-picking, and the beat.plp function uses predominant local pulse estimation. We implement both here and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e22f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an onset strength envelope for our full recording\n",
    "full_oe = librosa.onset.onset_strength(\n",
    "    y=full_mix.mean(axis=0),    # We need a mono file here, so average over both channels\n",
    "    sr=SAMPLE_RATE, \n",
    "    aggregate=np.median,    # The function used to aggregate overlapping frequencies in one frequency bin \n",
    "    fmax=11000,    # The maximum frequency to use, corresponding to our upper limit in Spleeter\n",
    "    center=False    # Do not center the tracked onsets on the envelope: this can cause issues with late annotation, see https://github.com/librosa/librosa/issues/1052\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b9738",
   "metadata": {},
   "source": [
    "### Beat tracking using dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the tracked beats\n",
    "_, full_be = librosa.beat.beat_track(\n",
    "    y=full_mix.T,    # Load in the full mix, transposed as necessary\n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    start_bpm=MEAN_TEMPO,    # 180 bpm approximately\n",
    "    trim=True,    # Trims weak beats from start/end of recording\n",
    "    tightness=TRACK_TIGHTNESS,    # Distribute beats tightly around the predicted tempo\n",
    "    units='time', \n",
    "    onset_envelope=full_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20228c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tracked beats against the onset envelope\n",
    "# TODO: implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b698ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay the tracked beats as clicks against the mixed audio file\n",
    "full_be_clicks = librosa.clicks(\n",
    "    full_be, \n",
    "    sr=SAMPLE_RATE, \n",
    "    length=full_mix.shape[1]\n",
    ")\n",
    "Audio(full_mix + full_be_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51e9c8",
   "metadata": {},
   "source": [
    "### Beat tracking using predominant local pulse estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the prior tempo distribution used in pulse estimation\n",
    "\n",
    "# By default, this is a uniform distribution over a defined minimum and maximum\n",
    "# tempo. However, we know that tempo in jazz is very tightly distributed, so\n",
    "# a uniform distribution doesn't seem appropriate. For now, I'm implementing\n",
    "# this using a normal distribution. However, we could investigate other\n",
    "# possible distributions in the full implementation, e.g. log-normal, \n",
    "# truncated normal...\n",
    "prior = stats.norm(loc=MEAN_TEMPO, scale=SD_TEMPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the predominant local pulse estimation, using our prior function\n",
    "pulse = librosa.beat.plp(\n",
    "    y=full_mix.mean(axis=0), \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH,\n",
    "    prior=prior\n",
    ")\n",
    "# Peak pick beats from our plp estimation, as audio frames\n",
    "beats_plp = np.flatnonzero(librosa.util.localmax(pulse))\n",
    "# Convert our peak frames to times\n",
    "beats_time = librosa.frames_to_time(beats_plp, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tracket beats against the onset envelope\n",
    "# TODO: implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163115a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay the tracked beats as clicks against the mixed audio file\n",
    "clicks_plp = librosa.clicks(\n",
    "    beats_time, \n",
    "    sr=SAMPLE_RATE, \n",
    "    length=full_mix.shape[1]\n",
    ")\n",
    "Audio(full_mix + clicks_plp, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840c488",
   "metadata": {},
   "source": [
    "To my ears, the dynamic programming implementation seems more consistent with where I 'feel' the pulse in the recording, and drifts less than the PLP estimate. We may wish to refine our choice of algorithm as we develop the full implementation, in particular investigating the functionality available in the 'prior' argument of beat.plp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020a10b",
   "metadata": {},
   "source": [
    "## 3. Detect onsets in the isolated tracks\n",
    "\n",
    "We now need to detect onsets in the individual, source-separated tracks for each performer. Note that this is a different process than the beat-tracking we've just implemented: what we're now trying to do is automatically pick up the position of *every note* that e.g. the pianist or bassist plays, rather than the periodic crotchet pulse.\n",
    "\n",
    "In the following code, we make use of the fmin= and fmax= arguments to restrict onset detection to a particular frequency band. These bands were ascertained from looking at the spectrograms of each recording to see which frequencies the desired instrument occupies. This process, we hope, should help mitigate the presence of any 'leaking' in the source-separated recordings -- e.g., if the bass leaks into the drum track, by omitting lower range frequencies this shouldn't effect the onset detection as much as processing the whole frequency band."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e2e9d",
   "metadata": {},
   "source": [
    "### Drum track ðŸ¥\n",
    "As the drum set consists of a variety of different drums and cymbals, we focus here on trying to detect onsets in the ride cymbal only. Our previous research has indicated that musicians typically understand this as the clearest indicator of tempo in a performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f70362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dual-mono track to mono\n",
    "drms = prediction['drums'].mean(axis=1)\n",
    "# Create our onset envelope from the track\n",
    "drms_oe = librosa.onset.onset_strength(\n",
    "    y=drms, \n",
    "    sr=SAMPLE_RATE, \n",
    "    center=False,\n",
    "    fmin=2500,    # The approximate minimum ride cymbal frequency\n",
    "    fmax=11000,    # The approximate maximum ride cymbal frequency\n",
    "    max_size=10,    # Size (in freqnuency bins) of the local maximum filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect onsets using the onset envelope\n",
    "drms_be = librosa.onset.onset_detect(\n",
    "    drms, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    units='time',\n",
    "    onset_envelope=drms_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay the detected onsets as clicks against the original isolated track\n",
    "drms_clicks = librosa.clicks(\n",
    "    drms_be, sr=SAMPLE_RATE, length=drms.shape[0]\n",
    ")\n",
    "Audio(drms + drms_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f52db0",
   "metadata": {},
   "source": [
    "### Piano track ðŸŽ¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = prediction['piano'].mean(axis=1)\n",
    "keys_oe = librosa.onset.onset_strength(\n",
    "    y=keys, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    center=False, \n",
    "    fmin=100, # The approximate minimum piano frequency\n",
    "    fmax=4000, # The approximate maximum piano frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29564ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_be = librosa.onset.onset_detect(\n",
    "    keys, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    units='time',\n",
    "    onset_envelope=keys_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_clicks = librosa.clicks(\n",
    "    keys_be, sr=SAMPLE_RATE, length=keys.shape[0]\n",
    ")\n",
    "Audio(keys + keys_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f33e38",
   "metadata": {},
   "source": [
    "### Bass track ðŸŽ»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bass = prediction['bass'].mean(axis=1)\n",
    "bass_oe = librosa.onset.onset_strength(\n",
    "    y=bass, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    fmin=50,    # The approximate minimum bass frequency\n",
    "    fmax=2000,    # The approximate maximum bass frequency \n",
    "    max_size=1100,   # This helps reduce the frequency of false detections\n",
    "    center=False, \n",
    "    detrend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bass_be = librosa.onset.onset_detect(\n",
    "    y=bass, \n",
    "    sr=SAMPLE_RATE, \n",
    "    units='time', \n",
    "    onset_envelope=bass_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bass_clicks = librosa.clicks(\n",
    "    bass_be, sr=SAMPLE_RATE, length=bass.shape[0]\n",
    ")\n",
    "Audio(bass + bass_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5da039",
   "metadata": {},
   "source": [
    "Ok, while those detected onsets definitely aren't perfectly aligned with the performances, they do capture enough information to work for now as a proof of concept. In particular, future refinements will aim to increase the precision of the piano (we're missing out quite a few onsets in the faster passages) and bass (again, a few onsets missed) detection. Note that any 'repeat' onsets -- e.g. where one bass note is captured twice in quick succession -- will be filtered later on. We may want to refine this in the future, so they're not present, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab395a0",
   "metadata": {},
   "source": [
    "## 4. Data preparation\n",
    "We now need to carry out a cleaning procedure to get our data into the correct form for modelling. This involves matching onsets detected in each individual isolated track with the metrical grid of crotchet beats extracted from the full stereo file, and "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38f457",
   "metadata": {},
   "source": [
    "### Match detected onsets with tracked metrical grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_matcher(\n",
    "    ons: np.float32, ins: np.array,\n",
    ") -> np.float32:\n",
    "    \"\"\"\n",
    "    This function tries to match an estimated crotchet beat position (estimated from the full stereo mix)\n",
    "    with the nearest onset played by one performer (estimated from the source-seperated track). \n",
    "    \n",
    "    If a close match cannot be found, i.e. it's above a set threshold, we set this data to missing. In this\n",
    "    case, either the onset detection algorithm did not pick up this onset, or the musician did not play\n",
    "    on that beat.\n",
    "    \n",
    "    By default, our threshold (set as a constant) is a 16th note, equivalent to 81 milliseconds at the\n",
    "    set tempo of 180 beats-per-minute.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get our closest match from our array of detected onsets\n",
    "    v = ins[np.abs(ins - ons).argmin()]\n",
    "    # Calculate our detection threshold\n",
    "    threshold = ((60 / MEAN_TEMPO) * 4) * DETECTION_THRESHOLD\n",
    "    # Return our onset if it's below our threshold, else return NaN\n",
    "    return v if np.abs(v - ons) <= threshold else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher_helper(\n",
    "    ons: float\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    A simple helper function that runs onset_matcher for a single onset for every performer, i.e. once\n",
    "    for bass, drums, and keys, and then returns a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'onset': ons,\n",
    "        'keys_match': onset_matcher(ons, keys_be),\n",
    "        'bass_match': onset_matcher(ons, bass_be),\n",
    "        'drms_match': onset_matcher(ons, drms_be)\n",
    "    }\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame([matcher_helper(ons) for ons in full_be])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e5fa5",
   "metadata": {},
   "source": [
    "### Coerce the data into the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bede10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_coerce(\n",
    "    y_ins: str, x_ins: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function simply coerces the data for one performer into the correct format by creating new columns.\n",
    "    The new columns correspond to:\n",
    "    \n",
    "    - instrument1_prev_ioi: the inter-onset interval between the current and the previous onset\n",
    "    - instrument1_next_ioi: the inter-onset interval between the current and the *next* onset\n",
    "    - instrument1_instrument2_asynchrony: the asynchrony between the two instruments at the current onset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compile the inter-onset intervals\n",
    "    dic = {\n",
    "        f'{y_ins}_prev_ioi': df[f'{y_ins}_match'].diff(),\n",
    "        f'{y_ins}_next_ioi': df[f'{y_ins}_match'].diff().shift(-1),\n",
    "    }\n",
    "    # Update the dictionary with our asynchrony values\n",
    "    dic.update({\n",
    "        f'{y_ins}_{ins}_asynchrony': df[f'{ins}_match'] - df[f'{y_ins}_match'] for ins in x_ins\n",
    "    })\n",
    "    # Return the dictionary as a dataframe\n",
    "    return pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83456f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate our original dataframe with the new columns we need each for the piano, drummer, and bassist\n",
    "df_conc = pd.concat(\n",
    "    [\n",
    "        df,\n",
    "        data_coerce('drms', ['keys', 'bass']),\n",
    "        data_coerce('keys', ['drms', 'bass']),\n",
    "        data_coerce('bass', ['drms', 'keys'])\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef0e59",
   "metadata": {},
   "source": [
    "## 5. Phase correction modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d40ad",
   "metadata": {},
   "source": [
    "### Generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(\n",
    "    y_ins: str, x_ins: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes in the name of one instrument (y_ins) and the remaining two instruments (x_ins), then\n",
    "    constructs the model in the required format and returns the OLSResults object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format our asynchrony (coupling) terms in the model\n",
    "    x_ins = '+'.join(f'{y_ins}_{ins}_asynchrony' for ins in x_ins)\n",
    "    # Format the rest of our model\n",
    "    md = f'{y_ins}_next_ioi~{y_ins}_prev_ioi+' + x_ins\n",
    "    # Create the regression model, fit to the data, and return\n",
    "    return smf.ols(md, data=df_conc).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our regression models for each instrument\n",
    "mds = [\n",
    "    gen_model('drms', ['keys', 'bass']),\n",
    "    gen_model('keys', ['drms', 'bass']),\n",
    "    gen_model('bass', ['keys', 'drms'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coupling responses from the model parameters\n",
    "coefs = {\n",
    "    p: c for md in mds for p, c in md.params.to_dict().items() if 'asynchrony' in p\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e36314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce the coupling responses into a dataframe\n",
    "df_coefs = pd.DataFrame(coefs.items(), columns=['ins', 'coupling'])\n",
    "df_coefs[['influenced', 'influencer', '_']] = df_coefs['ins'].str.split('_', expand=True)\n",
    "df_coefs = df_coefs.drop(columns=['ins', '_']).sort_values(by='influencer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6897f0",
   "metadata": {},
   "source": [
    "## 6. Visualise the coupling responses between musicians\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9432887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhaseCorrectionPlot:\n",
    "    \"\"\"\n",
    "    This class generates a plot where showing the measured couplings between each musician in the recording.\n",
    "    The direction of arrows indicates the influence and influencer instruments, namely the tendency of one performer\n",
    "    to follow (and adapt to) the indicated instrument: the thickeness and colour of the arrows indicate the strength\n",
    "    of this coupling.\n",
    "    \n",
    "    NB. This graph is similar to Figure 3b. in Jacoby et al. (2021)\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        self.df = kwargs.get('df', df_coefs)\n",
    "        self.fig, self.ax = plt.subplots(nrows=1, ncols=1, sharex=False, sharey=False, figsize=(10, 10))\n",
    "        self.ax.axis('off')\n",
    "        self.ax.set_aspect('equal')\n",
    "        self.colors = ['red', 'blue', 'green']\n",
    "        \n",
    "    def create_plot(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Called from outside the class to generate the required plot elements, show them, and save in the assets folder\n",
    "        \"\"\"\n",
    "        \n",
    "        self._add_musicians_images()\n",
    "        self._add_cover_image()\n",
    "        self._add_extra_text()\n",
    "        self._create_plot()\n",
    "        plt.show()\n",
    "        self.fig.savefig(r'.\\assets\\modelled_coupling.png', facecolor='white')\n",
    "        \n",
    "    def _create_plot(\n",
    "        self, arrow_mod: float = 25\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Creates the plot arrows and annotations, according to the modelled coupling responses.\n",
    "        The arrow_mod argument is the scaling applied to the coupling coefficient, used to set the correct width of the arrows.\n",
    "        \"\"\"\n",
    "        \n",
    "        instruments = ['bass', 'drms', 'keys']\n",
    "        # The starting coordinate for each arrow, i.e. the arrow tail\n",
    "        start_coords = [\n",
    "            [(0.35, 0.95), (0.725, 0.175)],\n",
    "            [(0.15, 0.325), (0.85, 0.325)],\n",
    "            [(0.65, 0.95), (0.275, 0.075)],\n",
    "        ]\n",
    "        # The end coordinate for each arrow, i.e. the arrow head\n",
    "        end_coords = [\n",
    "            [(0.05, 0.325), (0.275, 0.175)],\n",
    "            [(0.35, 0.75), (0.65, 0.75)],\n",
    "            [(0.95, 0.325), (0.725, 0.075)],\n",
    "        ]\n",
    "        \n",
    "        # Iterate over each influencer instrument, their respective arrows, and the color they're associated with\n",
    "        for influencer, start_coord, end_coord, col in zip(\n",
    "            instruments, start_coords, end_coords, self.colors\n",
    "        ):\n",
    "            # Iterate over each instrument they influence, and each individual arrow\n",
    "            for influenced, (x, y), (x2, y2) in zip(\n",
    "                [i for i in instruments if i != influencer], start_coord, end_coord\n",
    "            ):\n",
    "                # Get our coupling coefficient\n",
    "                subs = self._get_coupling_coefficient(influenced, influencer)\n",
    "                # Add in the arrow\n",
    "                self.ax.annotate(\n",
    "                    '', xy=(x, y), xycoords=self.ax.transAxes, \n",
    "                    xytext=(x2, y2), textcoords=self.ax.transAxes,\n",
    "                    arrowprops=dict(\n",
    "                        width=arrow_mod * subs, edgecolor=col, lw=1.5, facecolor=col, headwidth=20\n",
    "                    )\n",
    "                )\n",
    "                self._add_coupling_constant(subs, x, x2, y, y2)\n",
    "       \n",
    "    def _get_coupling_coefficient(\n",
    "        self, influenced: str, influencer: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Helper function to get the coupling coefficient between two instruments,\n",
    "        the influencer and influenced.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.df[\n",
    "            (self.df['influencer'] == influenced) & \n",
    "            (self.df['influenced'] == influencer)\n",
    "        ]['coupling'].iloc[0]\n",
    "    \n",
    "    \n",
    "    def _add_coupling_constant(\n",
    "        self, constant, x, x2, y, y2, mod: float = 0.03\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds coupling coefficient \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the default annotation position, the midpoint of our arrow\n",
    "        x_pos = (x + x2) / 2\n",
    "        y_pos = (y + y2) / 2\n",
    "        # Bottom of plot\n",
    "        if y_pos < 0.3:\n",
    "            y_pos += mod\n",
    "        # Top left of plot\n",
    "        elif x_pos < 0.5:\n",
    "            y_pos += mod\n",
    "            x_pos -= (mod * 1.1)\n",
    "        # Right of plot\n",
    "        elif x_pos > 0.5:\n",
    "            y_pos += mod\n",
    "            x_pos += (mod * 1.1)\n",
    "        # Add in the text using the x and y position\n",
    "        self.ax.text(\n",
    "            x_pos, y_pos, round(abs(constant), 2), ha='center', va='center', fontsize=14\n",
    "        )\n",
    "    \n",
    "    def _add_cover_image(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds in the album cover artwork to the plot \n",
    "        \"\"\"\n",
    "        \n",
    "        # Read the image and add it to the plot\n",
    "        img = plt.imread(r'.\\assets\\cover.jpg')\n",
    "        img = mpl.offsetbox.OffsetImage(img, zoom=0.5)\n",
    "        ab = mpl.offsetbox.AnnotationBbox(\n",
    "            img, (0.5, 0.5), xycoords='data', \n",
    "            bboxprops=dict(edgecolor='black', boxstyle='sawtooth', lw=2)\n",
    "        )\n",
    "        self.ax.add_artist(ab)\n",
    "        # Add in the caption to the album artwork\n",
    "        txt = \"\"\"$Peri's$ $Scope$ (1959)\\nRhythmic adaptation\\nbetween musicians\"\"\"\n",
    "        self.ax.text(0.5, 0.325, txt, ha='center', va='center', fontsize=15)\n",
    "    \n",
    "    def _add_musicians_images(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds images corresponding to each performer in the trio\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate through the position for each picture, the amount of zoom,\n",
    "        # the name of the performer, and the colour of the box around the picture\n",
    "        for (x, y), zoom, txt, col  in zip(\n",
    "            [(0.125, 0.125), (0.5, 0.875), (0.867, 0.133),],\n",
    "            [.68, .6375, .75,],\n",
    "            ['Scott LaFaro', 'Paul Motian', 'Bill Evans',],\n",
    "            self.colors\n",
    "        ):\n",
    "            # Get the filepath from the performer's name\n",
    "            mus = txt.split(' ')[0].lower()\n",
    "            # Display the image\n",
    "            img = plt.imread(fr'.\\assets\\{mus}.jpg')\n",
    "            img = mpl.offsetbox.OffsetImage(img, zoom=zoom)\n",
    "            ab = mpl.offsetbox.AnnotationBbox(\n",
    "                img, (x, y), xycoords='data', bboxprops=dict(edgecolor=col, lw=2)\n",
    "            )\n",
    "            self.ax.add_artist(ab)\n",
    "            # Add the text in, adjacent to the image\n",
    "            self.ax.text(\n",
    "                x, y + 0.15 if y < 0.5 else y - 0.15, \n",
    "                txt, ha='center', va='center', color=col\n",
    "            )\n",
    "            \n",
    "    def _add_extra_text(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds some additional extra text into the plot, e.g. titles, descriptions\n",
    "        \"\"\"\n",
    "        txt = \"\"\"Modelling interaction in the\\njazz rhythm section using \\nsource-seperated commercial\\naudio recordings\"\"\"\n",
    "        self.ax.text(-0.05, 0.9, txt, fontsize=12, ha='left', va='center', fontstyle='italic')\n",
    "        txt = \"\"\"Proof of Concept\"\"\"\n",
    "        self.ax.text(-0.05, 0.8, txt, fontsize=12, ha='left', va='center', fontweight='bold')\n",
    "        txt = \"\"\"Direction of arrows indicate the\\ntendency of one performer\\nto follow (adapt to) another;\\nthickness and colour of the\\narrows indicate coupling\\nstrength.\"\"\"\n",
    "        self.ax.text(1.05, 0.875, txt, fontsize=12, ha='right', va='center',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad444e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = PhaseCorrectionPlot(df=df_coefs)\n",
    "vis.create_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c252e",
   "metadata": {},
   "source": [
    "Even from the small, 30-second excerpt we've used here, the graph above shows some interesting details. Namely, both the bass and drums are tightly coupled to each other, but not to the piano soloist. The piano, meanwhile, couples strongly to both the bass and drums. In this sense, rhythmic adaptation is orientated towards the bass and drums comping, which provides an anchor for the piano improvisations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
