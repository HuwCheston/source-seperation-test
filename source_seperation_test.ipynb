{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673c7105",
   "metadata": {
    "id": "673c7105"
   },
   "source": [
    "# 🎹🎻🥁\n",
    "# Modelling interaction in the jazz rhythm section using source-seperated commercial audio recordings: proof of concept\n",
    "*Huw Cheston* (PhD Candidate @ Centre for Music & Science, University of Cambridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b780462",
   "metadata": {
    "id": "7b780462"
   },
   "source": [
    "## Procedure\n",
    "\n",
    "0. Import-dependencies and set constants\n",
    "1. Load in the stereo audio file and separate it into individual tracks for the piano, bass, and drums performance.\n",
    "    - Here, we use a 30-second excerpt from \"Peri's Scope\" by the Bill Evans Trio, from the 1959 album Portrait in Jazz. The excerpt comes from the middle of Bill Evans' piano solo.\n",
    "2. Generate a tempo map from the complete stereo audio file\n",
    "    - This map corresponds to the estimated position of each crotchet onset\n",
    "3. Detect individual onsets in the isolated tracks\n",
    "    - This is equivalent to detecting the position of *every single note* by a performer\n",
    "4. Coerce the data into the correct format for modelling\n",
    "    - This involves matching the estimated position of each crotchet with the nearest onset by each performer\n",
    "5. Generate the phase correction model and extract the coupling coefficients\n",
    "6. Visualise the degree of coupling between the musicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350fffb",
   "metadata": {
    "id": "3350fffb"
   },
   "source": [
    "## 0. Import dependencies\n",
    "\n",
    "The following few cells load in our required source separation, signal processing, and data analysis libraries, and set a few constant variables that we refer to throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9NrfLDVzd2Sn",
   "metadata": {
    "id": "9NrfLDVzd2Sn"
   },
   "outputs": [],
   "source": [
    "# We'll make a few directories here to store our colab outputs\n",
    "!mkdir ./output/\n",
    "!mkdir ./assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F-9uLrzI7xQW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-9uLrzI7xQW",
    "outputId": "53bf5cc0-21a2-490f-ff03-7c707f8bfdab"
   },
   "outputs": [],
   "source": [
    "# First, we need to install Spleeter and its dependency FFmpeg,\n",
    "# These don't come packaged in Colab by default\n",
    "!apt install ffmpeg\n",
    "!pip install spleeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c134b9",
   "metadata": {
    "id": "36c134b9"
   },
   "outputs": [],
   "source": [
    "from spleeter.separator import Separator\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406af0ef",
   "metadata": {
    "id": "406af0ef"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3a8de",
   "metadata": {
    "id": "f6d3a8de"
   },
   "outputs": [],
   "source": [
    "# Audio attributes\n",
    "SAMPLE_RATE = 44100\n",
    "HOP_LENGTH = 512\n",
    "INPUT_AUDIO = \"input.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a74645",
   "metadata": {
    "id": "74a74645"
   },
   "outputs": [],
   "source": [
    "# Spleeter model - vocals/piano/bass/drums/other\n",
    "MODEL = \"spleeter:5stems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dddab5",
   "metadata": {
    "id": "92dddab5"
   },
   "outputs": [],
   "source": [
    "# Track details\n",
    "MEAN_TEMPO = 180\n",
    "MIN_TEMPO, MAX_TEMPO = 175, 185\n",
    "SD_TEMPO = 0.1\n",
    "TRACK_TIGHTNESS = 2000\n",
    "DETECTION_THRESHOLD = 1/12    # A triplet quaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413b020",
   "metadata": {
    "id": "c413b020"
   },
   "source": [
    "## 1. Source seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a90b67",
   "metadata": {
    "id": "a2a90b67"
   },
   "source": [
    "### Load the audio and define the Spleeter model\n",
    "[*Spleeter*](https://github.com/deezer/spleeter) is a source-separation Python library created by the streaming service Deezer. It is currently used in both [iZotope RX](https://www.izotope.com/en/products/rx.html) and [Steinberg Spectralayers](https://www.steinberg.net/spectralayers/). Currently, it is placed 16th out of 25 entries on the [MUSDB18 Source Separation leaderboard.](https://paperswithcode.com/sota/music-source-separation-on-musdb18)\n",
    "\n",
    "Spleeter contains a variety of pre-trained machine learning models, capable of extracting various combinations of instruments. Here, we use the [5stems model](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-5stems-model) to extract piano, bass, and drums from the stereo recording, discarding the vocals and other (residual) tracks. By default, Spleeter also only performs separation up to 11kHz; however, with [some modification to the internal configuration files](https://github.com/deezer/spleeter/wiki/5.-FAQ#why-are-there-no-high-frequencies-in-the-generated-output-files-) (not ideal for running online!) it can be used up to 22kHz. For now, we just use the defaults, with the caveat that we may increase the sampling rate in the full implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qes1ROPx_SlE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qes1ROPx_SlE",
    "outputId": "f0bb1a97-906e-43b1-ccc3-5517f636be37"
   },
   "outputs": [],
   "source": [
    "# First, we need to pull in the audio file we're using for analysis from GitHub\n",
    "!wget -O input.mp3 https://github.com/HuwCheston/source-seperation-test/raw/main/input.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955bcb7",
   "metadata": {
    "id": "d955bcb7"
   },
   "outputs": [],
   "source": [
    "# Now we can load the full mix of the audio file in to Librosa.\n",
    "full_mix, _ = librosa.load(\n",
    "    INPUT_AUDIO,    # The filepath to our audio\n",
    "    sr=SAMPLE_RATE,    # Our sample rate\n",
    "    mono=False,    # Load the file in stereo, as we need both channels\n",
    "    offset=0,    # Load the file from the very beginning\n",
    "    duration=None,    # Don't trim the audio file  \n",
    "    dtype=np.float32    # Set the data type as a float for use in numpy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1549dd",
   "metadata": {
    "id": "de1549dd"
   },
   "outputs": [],
   "source": [
    "# Now, we instantiate our Spleeter Separator instance.\n",
    "# We use the model we specified as a constant earlier (i.e. 5stems)\n",
    "separator = Separator(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd835f81",
   "metadata": {
    "id": "bd835f81"
   },
   "source": [
    "### Seperate the audio\n",
    "One of the advantages of studying piano trio recordings over larger ensembles is that recording engineers typically used the stereo spectrum in a fairly predictable way. This knowledge of how instruments are panned can help us in source separation. For instance, in this recording, Scott LaFaro's bass is panned over to the right channel, Paul Motian's drums are on the left channel, and Bill Evans' piano is in the centre. We therefore can apply source separation individually to these channels to get the cleanest signal for each instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e185d66",
   "metadata": {
    "id": "2e185d66"
   },
   "outputs": [],
   "source": [
    "def extract_channel(\n",
    "    waveform: np.array, channel: int\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    This function extracts the left or right channel from a stereo audio source.\n",
    "    \"\"\"\n",
    "    return np.tile(waveform[:, channel].T.reshape(-1, 1), (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93463958",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93463958",
    "outputId": "d4ed7b3a-f0e6-414f-f741-8bca4c4f0d21"
   },
   "outputs": [],
   "source": [
    "# Instantiate an empty dictionary to store our audio in\n",
    "prediction = {}\n",
    "# We apply source separation to the left channel only, for the drums\n",
    "prediction['drums'] = separator.separate(extract_channel(full_mix.T, 0))['drums']\n",
    "# ... and on the right channel for bass\n",
    "prediction['bass'] = separator.separate(extract_channel(full_mix.T, 1))['bass']\n",
    "# We use both channels for keys (these are centre panned)\n",
    "prediction['piano'] = separator.separate(full_mix.T)['piano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79e1a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "0c79e1a2",
    "outputId": "2fd095c0-cb4a-4117-87d6-cf5fb8d7e79c"
   },
   "outputs": [],
   "source": [
    "# Let's listen to our drums extracted audio to make sure it sounds reasonable\n",
    "Audio(prediction['drums'].T, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f212b",
   "metadata": {
    "id": "383f212b"
   },
   "source": [
    "That sounds pretty good to me for now; while we occasionally get a bit of leakage from both the piano (0:12-0:15) and the bass (0:19-0:20), the ride cymbal, snare drum, and bass drum are all cutting through cleanly in the isolated drum track. We'll refine the source separation further in the full implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1689127",
   "metadata": {
    "id": "f1689127"
   },
   "source": [
    "## 2. Beat tracking from the full mix 🎹🎻🥁\n",
    "\n",
    "To get a general idea of where each beat is in the complete ensemble recording, we now automatically generate a tempo map of the performance using beat tracking. We'll use this to match up the onsets detected in the individual piano, bass, and drum tracks with each crotchet beat in the performance.\n",
    "\n",
    "Librosa implements two algorithms for beat tracking: the beat.beat_track function uses dynamic programming and peak-picking, and the beat.plp function uses predominant local pulse estimation. We implement both here and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e22f4b",
   "metadata": {
    "id": "72e22f4b"
   },
   "outputs": [],
   "source": [
    "# Generate an onset strength envelope for our full recording\n",
    "full_oe = librosa.onset.onset_strength(\n",
    "    y=full_mix.mean(axis=0),    # We need a mono file here, so average over both channels\n",
    "    sr=SAMPLE_RATE, \n",
    "    aggregate=np.median,    # The function used to aggregate overlapping frequencies in one frequency bin \n",
    "    fmax=11000,    # The maximum frequency to use, corresponding to our upper limit in Spleeter\n",
    "    center=False    # Do not center the tracked onsets on the envelope: this can cause issues with late annotation, see https://github.com/librosa/librosa/issues/1052\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b9738",
   "metadata": {
    "id": "429b9738"
   },
   "source": [
    "### Beat tracking using dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40d85c",
   "metadata": {
    "id": "4c40d85c"
   },
   "outputs": [],
   "source": [
    "# Generate the tracked beats\n",
    "_, full_be = librosa.beat.beat_track(\n",
    "    y=full_mix.T,    # Load in the full mix, transposed as necessary\n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    start_bpm=MEAN_TEMPO,    # 180 bpm approximately\n",
    "    trim=True,    # Trims weak beats from start/end of recording\n",
    "    tightness=TRACK_TIGHTNESS,    # Distribute beats tightly around the predicted tempo\n",
    "    units='time', \n",
    "    onset_envelope=full_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b698ef4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "3b698ef4",
    "outputId": "a6594921-b8d2-4551-db9b-d4cbd264b876"
   },
   "outputs": [],
   "source": [
    "# Overlay the tracked beats as clicks against the mixed audio file\n",
    "full_be_clicks = librosa.clicks(\n",
    "    full_be, \n",
    "    sr=SAMPLE_RATE, \n",
    "    length=full_mix.shape[1]\n",
    ")\n",
    "Audio(full_mix + full_be_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51e9c8",
   "metadata": {
    "id": "9b51e9c8"
   },
   "source": [
    "### Beat tracking using predominant local pulse estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5b90f",
   "metadata": {
    "id": "45b5b90f"
   },
   "outputs": [],
   "source": [
    "# Define a function for the prior tempo distribution used in pulse estimation\n",
    "\n",
    "# By default, this is a uniform distribution over a defined minimum and maximum\n",
    "# tempo. However, we know that tempo in jazz is very tightly distributed, so\n",
    "# a uniform distribution doesn't seem appropriate. For now, I'm implementing\n",
    "# this using a normal distribution. However, we could investigate other\n",
    "# possible distributions in the full implementation, e.g. log-normal, \n",
    "# truncated normal...\n",
    "prior = stats.norm(loc=MEAN_TEMPO, scale=SD_TEMPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa0fd1",
   "metadata": {
    "id": "94fa0fd1"
   },
   "outputs": [],
   "source": [
    "# Generate the predominant local pulse estimation, using our prior function\n",
    "pulse = librosa.beat.plp(\n",
    "    y=full_mix.mean(axis=0), \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH,\n",
    "    prior=prior\n",
    ")\n",
    "# Peak pick beats from our plp estimation, as audio frames\n",
    "beats_plp = np.flatnonzero(librosa.util.localmax(pulse))\n",
    "# Convert our peak frames to times\n",
    "beats_time = librosa.frames_to_time(beats_plp, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163115a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "163115a6",
    "outputId": "29800d4a-4aba-43b2-b6a3-118eae4ab53a"
   },
   "outputs": [],
   "source": [
    "# Overlay the tracked beats as clicks against the mixed audio file\n",
    "clicks_plp = librosa.clicks(\n",
    "    beats_time, \n",
    "    sr=SAMPLE_RATE, \n",
    "    length=full_mix.shape[1]\n",
    ")\n",
    "Audio(full_mix + clicks_plp, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KqE6HOdjeKZR",
   "metadata": {
    "id": "KqE6HOdjeKZR"
   },
   "source": [
    "### Visualise the difference between the two algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0tR2XvEAJp-Q",
   "metadata": {
    "id": "0tR2XvEAJp-Q"
   },
   "outputs": [],
   "source": [
    "class OnsetStrengthTrackedBeatsPlot:\n",
    "  \"\"\"\n",
    "  Creates a plot that compares the accuracy of detected beats between two \n",
    "  methods of pulse estimation - a dynamic programming and predominant local\n",
    "  pulse estimation algorithm. Also shows a spectrogram of the audio for \n",
    "  comparison to the osnet strength.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, tracked_beats_dp, tracked_beats_plp, **kwargs):\n",
    "    self.onset_envelope = full_oe\n",
    "    self.tracked_beats_dp = tracked_beats_dp\n",
    "    self.tracked_beats_plp = tracked_beats_plp\n",
    "    self.x_cutoff = kwargs.get('x_cutoff', 10)\n",
    "    self.fig, self.ax = plt.subplots(\n",
    "        nrows=2, ncols=1, figsize=(15, 8), sharex=True, sharey=False\n",
    "    )\n",
    "\n",
    "  def _format_x_axis(\n",
    "      self\n",
    "  ) -> np.array:\n",
    "    \"\"\"\n",
    "    Returns x-axis times with correct length, according to cutoff\n",
    "    \"\"\"\n",
    "    \n",
    "    x = librosa.times_like(\n",
    "        self.onset_envelope, sr=SAMPLE_RATE, axis=-1, hop_length=HOP_LENGTH\n",
    "    )\n",
    "    return np.array([time for time in x if time < self.x_cutoff])\n",
    "\n",
    "  def create_plot(\n",
    "      self\n",
    "  ) -> None:\n",
    "    \"\"\"\n",
    "    Called from outside the class, generates the plot and saves\n",
    "    \"\"\"\n",
    "\n",
    "    self._create_spectrogram()\n",
    "    self._create_lineplot()\n",
    "    self._format_ax()\n",
    "    self._format_fig()\n",
    "    self.fig.savefig(r'./output/beat_tracking.png', facecolor='white')\n",
    "\n",
    "  def _create_spectrogram(\n",
    "    self\n",
    "  ) -> None:\n",
    "    \"\"\"\n",
    "    Creates the spectrogram on the upper axis\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the desired number of samples for the track, with the cutoff\n",
    "    shape = int((SAMPLE_RATE * self.x_cutoff))\n",
    "    # Trim the duration of the track and convert to mono\n",
    "    track = np.resize(full_mix, (2, shape)).mean(axis=0)\n",
    "    # Get the amplitude (in decibels) of the track\n",
    "    d = librosa.amplitude_to_db(np.abs(librosa.stft(track)), ref=np.max)\n",
    "    # Show the spectrogram\n",
    "    spec = librosa.display.specshow(\n",
    "        d, sr=SAMPLE_RATE, hop_length=HOP_LENGTH, \n",
    "        x_axis='time', y_axis='log', ax=self.ax[0]\n",
    "    )\n",
    "    # Add the colorbar in, on the right of the figure (next to the axis)\n",
    "    cax = self.fig.add_axes([0.82, 0.54, 0.01, 0.335])\n",
    "    self.fig.colorbar(spec, cax=cax, orientation='vertical', format=\"%+2.f dB\")\n",
    "\n",
    "  def _create_lineplot(\n",
    "      self, \n",
    "  ) -> None:\n",
    "    \"\"\"\n",
    "    Creates the lineplot (onset envelope and detected beats) on the lower axis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot our onset envelope, trimmed to the desired length\n",
    "    x = self._format_x_axis()\n",
    "    y = self.onset_envelope[:x.shape[0]]\n",
    "    self.ax[1].plot(x, y)\n",
    "    # Overlay our tracked beats as vertical lines\n",
    "    for tb, col, lab in zip(\n",
    "        [self.tracked_beats_dp, self.tracked_beats_plp], \n",
    "        ['red', 'green'], ['DP', 'PLP']\n",
    "    ):\n",
    "        # Get only the beats before the cutoff\n",
    "        lines = np.array([line for line in tb if line < self.x_cutoff])\n",
    "        # Add the vertical lines onto the plot with the correct color and label\n",
    "        self.ax[1].vlines(\n",
    "            lines, ymin=0, ymax=y.max(), color=col, ls='--', label=lab\n",
    "        )\n",
    "    # Add the legend in and align with the upper axis colorbar\n",
    "    self.fig.legend(bbox_to_anchor=(0.91, 0.375), frameon=False, title='Method')\n",
    "\n",
    "  def _format_ax(\n",
    "      self\n",
    "  ) -> None:\n",
    "    \"\"\"\n",
    "    Sets axis characteristics, e.g. title, labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Modify upper axis, the spectrogram\n",
    "    self.ax[0].set(\n",
    "        ylim=(0, 16384), xlabel='', title='Log-frequency power spectrogram'\n",
    "    )\n",
    "    # Modify lower axis, the line plot\n",
    "    self.ax[1].set(title='Beat tracking')\n",
    "    # Apply modifications to both axis\n",
    "    for ax, lab in zip(self.ax.flatten(), ['Frequency (Hz)', 'Onset strength']):\n",
    "        ax.set(xticks=np.linspace(0, self.x_cutoff, 6))\n",
    "        ax.set_ylabel(lab, fontsize=18)\n",
    "\n",
    "  def _format_fig(\n",
    "      self\n",
    "  ) -> None:\n",
    "    \"\"\"\n",
    "    Sets figure-level attributes\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjust plot positioning slightly\n",
    "    self.fig.subplots_adjust(right=0.8)\n",
    "    # Add the x-axis label to the bottom of the plot\n",
    "    self.fig.supxlabel('Time (s)', x=0.45, y=0.05, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suj7ds8He-Ur",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "suj7ds8He-Ur",
    "outputId": "35987ac2-1312-4dbc-c66c-3e5eea5ee9a2"
   },
   "outputs": [],
   "source": [
    "os_ = OnsetStrengthTrackedBeatsPlot(\n",
    "    tracked_beats_dp=full_be, tracked_beats_plp=beats_time\n",
    ")\n",
    "os_.create_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840c488",
   "metadata": {
    "id": "7840c488"
   },
   "source": [
    "Ok, so it looks like the dynamic programming algorithm is generally closer to the onset strength peaks we can perceive visually in the above graph. Going forwards, we'll use the results from this algorithm. We may wish to refine our choice of algorithm (and the arguments we provide it) as we develop the full implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020a10b",
   "metadata": {
    "id": "8020a10b"
   },
   "source": [
    "## 3. Detect onsets in the isolated tracks\n",
    "\n",
    "We now need to detect onsets in the individual, source-separated tracks for each performer. Note that this is a different process than the beat-tracking we've just implemented: what we're now trying to do is automatically pick up the position of *every note* that e.g. the pianist or bassist plays, rather than the periodic crotchet pulse.\n",
    "\n",
    "In the following code, we make use of the fmin= and fmax= arguments to restrict onset detection to a particular frequency band. These bands were ascertained from looking at the spectrograms of each recording to see which frequencies the desired instrument occupies. This process, we hope, should help mitigate the presence of any 'leaking' in the source-separated recordings -- e.g., if the bass leaks into the drum track, by omitting lower range frequencies this shouldn't effect the onset detection as much as processing the whole frequency band."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e2e9d",
   "metadata": {
    "id": "4b5e2e9d"
   },
   "source": [
    "### Drum track 🥁\n",
    "As the drum set consists of a variety of different drums and cymbals, we focus here on trying to detect onsets in the ride cymbal only. Our previous research has indicated that musicians typically understand this as the clearest indicator of tempo in a performance. In this recording, the ride cymbal sits in the upper frequency band, from about 2500 to 11000 (the limit of the Spleeter output) Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f70362",
   "metadata": {
    "id": "66f70362"
   },
   "outputs": [],
   "source": [
    "# Convert our dual-mono track to mono\n",
    "drms = prediction['drums'].mean(axis=1)\n",
    "# Create our onset envelope from the track\n",
    "drms_oe = librosa.onset.onset_strength(\n",
    "    y=drms, \n",
    "    sr=SAMPLE_RATE, \n",
    "    center=False,\n",
    "    fmin=2500,    # The approximate minimum ride cymbal frequency\n",
    "    fmax=11000,    # The approximate maximum ride cymbal frequency\n",
    "    max_size=10,    # Size (in freqnuency bins) of the local maximum filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df4bf8",
   "metadata": {
    "id": "02df4bf8"
   },
   "outputs": [],
   "source": [
    "# Detect onsets using the onset envelope\n",
    "drms_be = librosa.onset.onset_detect(\n",
    "    drms, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    units='time',\n",
    "    onset_envelope=drms_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1d1a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "87c1d1a2",
    "outputId": "8401c218-2003-4ecf-d04a-1f5d228d22dc"
   },
   "outputs": [],
   "source": [
    "# Overlay the detected onsets as clicks against the original isolated track\n",
    "drms_clicks = librosa.clicks(\n",
    "    drms_be, sr=SAMPLE_RATE, length=drms.shape[0]\n",
    ")\n",
    "Audio(drms + drms_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f52db0",
   "metadata": {
    "id": "09f52db0"
   },
   "source": [
    "### Piano track 🎹\n",
    "We concentrate on the mid-range frequency band, from 100 to 4000 Hz, when detecting onsets in the piano performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85e620",
   "metadata": {
    "id": "dc85e620"
   },
   "outputs": [],
   "source": [
    "keys = prediction['piano'].mean(axis=1)\n",
    "keys_oe = librosa.onset.onset_strength(\n",
    "    y=keys, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    center=False, \n",
    "    fmin=100, # The approximate minimum piano frequency\n",
    "    fmax=4000, # The approximate maximum piano frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29564ed",
   "metadata": {
    "id": "f29564ed"
   },
   "outputs": [],
   "source": [
    "keys_be = librosa.onset.onset_detect(\n",
    "    keys, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    units='time',\n",
    "    onset_envelope=keys_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8a4da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "93b8a4da",
    "outputId": "ae24c975-b07b-40fd-e4a6-e6ab76ccf58b"
   },
   "outputs": [],
   "source": [
    "keys_clicks = librosa.clicks(\n",
    "    keys_be, sr=SAMPLE_RATE, length=keys.shape[0]\n",
    ")\n",
    "Audio(keys + keys_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f33e38",
   "metadata": {
    "id": "09f33e38"
   },
   "source": [
    "### Bass track 🎻\n",
    "Most of the energy for the bass is concentrated in the lower frequency range, from about 50 to 2000 Hz, so we can concentrate the onset detection on this region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadbd1f",
   "metadata": {
    "id": "cfadbd1f"
   },
   "outputs": [],
   "source": [
    "bass = prediction['bass'].mean(axis=1)\n",
    "bass_oe = librosa.onset.onset_strength(\n",
    "    y=bass, \n",
    "    sr=SAMPLE_RATE, \n",
    "    hop_length=HOP_LENGTH, \n",
    "    fmin=50,    # The approximate minimum bass frequency\n",
    "    fmax=2000,    # The approximate maximum bass frequency \n",
    "    max_size=1100,   # This helps reduce the frequency of false detections\n",
    "    center=False, \n",
    "    detrend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164fcc4",
   "metadata": {
    "id": "e164fcc4"
   },
   "outputs": [],
   "source": [
    "bass_be = librosa.onset.onset_detect(\n",
    "    y=bass, \n",
    "    sr=SAMPLE_RATE, \n",
    "    units='time', \n",
    "    onset_envelope=bass_oe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ce36d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "c51ce36d",
    "outputId": "5f69b340-ee85-47b8-f461-fc6eef7ab3fc"
   },
   "outputs": [],
   "source": [
    "bass_clicks = librosa.clicks(\n",
    "    bass_be, sr=SAMPLE_RATE, length=bass.shape[0]\n",
    ")\n",
    "Audio(bass + bass_clicks, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5da039",
   "metadata": {
    "id": "8b5da039"
   },
   "source": [
    "Ok, while those detected onsets definitely aren't perfectly aligned with the performances, they do capture enough information to work for now as a proof of concept. In particular, future refinements will aim to increase the precision of the piano (we're missing out quite a few onsets in the faster passages) and bass (again, a few onsets missed) detection. Note that any 'repeat' onsets -- e.g. where one bass note is captured twice in quick succession -- will be filtered later on. We may want to refine this in the future, so they're not present, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab395a0",
   "metadata": {
    "id": "4ab395a0"
   },
   "source": [
    "## 4. Data preparation\n",
    "We now need to carry out a cleaning procedure to get our data into the correct form for modelling. This involves matching onsets detected in each individual isolated track with the metrical grid of crotchet beats extracted from the full stereo file, and coercing the data into the correct format for modelling (e.g. by extracting inter-onset intervals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38f457",
   "metadata": {
    "id": "2a38f457"
   },
   "source": [
    "### Match detected onsets with tracked metrical grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88d60e",
   "metadata": {
    "id": "ac88d60e"
   },
   "outputs": [],
   "source": [
    "def onset_matcher(\n",
    "    ons: np.float32, ins: np.array,\n",
    ") -> np.float32:\n",
    "    \"\"\"\n",
    "    This function tries to match an estimated crotchet beat position (estimated from the full stereo mix)\n",
    "    with the nearest onset played by one performer (estimated from the source-seperated track). \n",
    "    \n",
    "    If a close match cannot be found, i.e. it's above a set threshold, we set this data to missing. In this\n",
    "    case, either the onset detection algorithm did not pick up this onset, or the musician did not play\n",
    "    on that beat.\n",
    "    \n",
    "    By default, our threshold (set as a constant) is a 16th note, equivalent to 81 milliseconds at the\n",
    "    set tempo of 180 beats-per-minute.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get our closest match from our array of detected onsets\n",
    "    v = ins[np.abs(ins - ons).argmin()]\n",
    "    # Calculate our detection threshold\n",
    "    threshold = ((60 / MEAN_TEMPO) * 4) * DETECTION_THRESHOLD\n",
    "    # Return our onset if it's below our threshold, else return NaN\n",
    "    return v if np.abs(v - ons) <= threshold else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebc47a",
   "metadata": {
    "id": "faebc47a"
   },
   "outputs": [],
   "source": [
    "def matcher_helper(\n",
    "    ons: float\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    A simple helper function that runs onset_matcher for a single onset for every performer, i.e. once\n",
    "    for bass, drums, and keys, and then returns a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'onset': ons,\n",
    "        'keys_match': onset_matcher(ons, keys_be),\n",
    "        'bass_match': onset_matcher(ons, bass_be),\n",
    "        'drms_match': onset_matcher(ons, drms_be)\n",
    "    }\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame([matcher_helper(ons) for ons in full_be])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e5fa5",
   "metadata": {
    "id": "1b2e5fa5"
   },
   "source": [
    "### Coerce the data into the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bede10",
   "metadata": {
    "id": "e0bede10"
   },
   "outputs": [],
   "source": [
    "def data_coerce(\n",
    "    y_ins: str, x_ins: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function simply coerces the data for one performer into the correct format by creating new columns.\n",
    "    The new columns correspond to:\n",
    "    \n",
    "    - instrument1_prev_ioi: the inter-onset interval between the current and the previous onset\n",
    "    - instrument1_next_ioi: the inter-onset interval between the current and the *next* onset\n",
    "    - instrument1_instrument2_asynchrony: the asynchrony between the two instruments at the current onset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compile the inter-onset intervals\n",
    "    dic = {\n",
    "        f'{y_ins}_prev_ioi': df[f'{y_ins}_match'].diff(),\n",
    "        f'{y_ins}_next_ioi': df[f'{y_ins}_match'].diff().shift(-1),\n",
    "    }\n",
    "    # Update the dictionary with our asynchrony values\n",
    "    dic.update({\n",
    "        f'{y_ins}_{ins}_asynchrony': df[f'{ins}_match'] - df[f'{y_ins}_match'] for ins in x_ins\n",
    "    })\n",
    "    # Return the dictionary as a dataframe\n",
    "    return pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83456f37",
   "metadata": {
    "id": "83456f37"
   },
   "outputs": [],
   "source": [
    "# Concatenate our original dataframe with the new columns we need each for the piano, drummer, and bassist\n",
    "df_conc = pd.concat(\n",
    "    [\n",
    "        df,\n",
    "        data_coerce('drms', ['keys', 'bass']),\n",
    "        data_coerce('keys', ['drms', 'bass']),\n",
    "        data_coerce('bass', ['drms', 'keys'])\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef0e59",
   "metadata": {
    "id": "4cef0e59"
   },
   "source": [
    "## 5. Phase correction modelling\n",
    "Now we have the data in the correct format, we can begin to create our model. This is a linear phase correction model (Vorberg & Wing, 1996), which predicts the duration of future inter-onset intervals by a performer from both the duration of prior inter-onset intervals by that same performer and the asynchrony between their’s and their partner’s previous onset (Jacoby et al., 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d40ad",
   "metadata": {
    "id": "8c5d40ad"
   },
   "source": [
    "### Generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589ae98",
   "metadata": {
    "id": "e589ae98"
   },
   "outputs": [],
   "source": [
    "def gen_model(\n",
    "    y_ins: str, x_ins: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes in the name of one instrument (y_ins) and the remaining two instruments (x_ins), then\n",
    "    constructs the model in the required format and returns the OLSResults object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format our asynchrony (coupling) terms in the model\n",
    "    x_ins = '+'.join(f'{y_ins}_{ins}_asynchrony' for ins in x_ins)\n",
    "    # Format the rest of our model\n",
    "    md = f'{y_ins}_next_ioi~{y_ins}_prev_ioi+' + x_ins\n",
    "    # Create the regression model, fit to the data, and return\n",
    "    return smf.ols(md, data=df_conc).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb731d",
   "metadata": {
    "id": "1ceb731d"
   },
   "outputs": [],
   "source": [
    "# Generate our regression models for each instrument\n",
    "mds = [\n",
    "    gen_model('drms', ['keys', 'bass']),\n",
    "    gen_model('keys', ['drms', 'bass']),\n",
    "    gen_model('bass', ['keys', 'drms'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99a830",
   "metadata": {
    "id": "ca99a830"
   },
   "outputs": [],
   "source": [
    "# Extract the coupling responses from the model parameters\n",
    "coefs = {\n",
    "    p: c for md in mds for p, c in md.params.to_dict().items() if 'asynchrony' in p\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e36314",
   "metadata": {
    "id": "87e36314"
   },
   "outputs": [],
   "source": [
    "# Coerce the coupling responses into a dataframe\n",
    "df_coefs = pd.DataFrame(coefs.items(), columns=['ins', 'coupling'])\n",
    "df_coefs[['influenced', 'influencer', '_']] = df_coefs['ins'].str.split('_', expand=True)\n",
    "df_coefs = df_coefs.drop(columns=['ins', '_']).sort_values(by='influencer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6897f0",
   "metadata": {
    "id": "5e6897f0"
   },
   "source": [
    "## 6. Visualise the coupling responses between musicians\n",
    "We now create a graph that shows visually the modelled coupling responses between the three musicians. This plot is similar to Figure 3b. in Jacoby et al (2021). The direction of arrows on our plot indicates the influence and influencer instruments, namely the tendency of one performer to follow (and adapt to) the indicated instrument: the thickness and colour of the arrows indicate the strength of this coupling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XJvWJULL_PT2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJvWJULL_PT2",
    "outputId": "2d4f3bb9-0fde-478f-ecda-e2b01c1dd285"
   },
   "outputs": [],
   "source": [
    "# Here, we pull in the image files we're going to use to display on our graph\n",
    "!wget -O ./assets/bill.jpg https://github.com/HuwCheston/source-seperation-test/raw/main/assets/bill.jpg\n",
    "!wget -O ./assets/scott.jpg https://github.com/HuwCheston/source-seperation-test/raw/main/assets/scott.jpg\n",
    "!wget -O ./assets/paul.jpg  https://github.com/HuwCheston/source-seperation-test/raw/main/assets/paul.jpg\n",
    "!wget -O ./assets/cover.jpg  https://github.com/HuwCheston/source-seperation-test/raw/main/assets/cover.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9432887",
   "metadata": {
    "id": "b9432887"
   },
   "outputs": [],
   "source": [
    "class PhaseCorrectionPlot:\n",
    "    \"\"\"\n",
    "    This class generates a plot where showing the measured couplings between each musician in the recording.\n",
    "    The direction of arrows indicates the influence and influencer instruments, namely the tendency of one performer\n",
    "    to follow (and adapt to) the indicated instrument: the thickeness and colour of the arrows indicate the strength\n",
    "    of this coupling.\n",
    "    \n",
    "    NB. This graph is similar to Figure 3b. in Jacoby et al. (2021)\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        self.df = kwargs.get('df', df_coefs)\n",
    "        self.fig, self.ax = plt.subplots(nrows=1, ncols=1, sharex=False, sharey=False, figsize=(10, 10))\n",
    "        self.ax.axis('off')\n",
    "        self.ax.set_aspect('equal')\n",
    "        self.colors = ['red', 'blue', 'green']\n",
    "        \n",
    "    def create_plot(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Called from outside the class to generate the required plot elements, show them, and save in the assets folder\n",
    "        \"\"\"\n",
    "        \n",
    "        self._add_musicians_images()\n",
    "        self._add_cover_image()\n",
    "        self._add_extra_text()\n",
    "        self._create_plot()\n",
    "        plt.show()\n",
    "        self.fig.savefig(r'./output/modelled_coupling.png', facecolor='white')\n",
    "        \n",
    "    def _create_plot(\n",
    "        self, arrow_mod: float = 25\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Creates the plot arrows and annotations, according to the modelled coupling responses.\n",
    "        The arrow_mod argument is the scaling applied to the coupling coefficient, used to set the correct width of the arrows.\n",
    "        \"\"\"\n",
    "        \n",
    "        instruments = ['bass', 'drms', 'keys']\n",
    "        # The starting coordinate for each arrow, i.e. the arrow tail\n",
    "        start_coords = [\n",
    "            [(0.35, 0.95), (0.725, 0.175)],\n",
    "            [(0.15, 0.325), (0.85, 0.325)],\n",
    "            [(0.65, 0.95), (0.275, 0.075)],\n",
    "        ]\n",
    "        # The end coordinate for each arrow, i.e. the arrow head\n",
    "        end_coords = [\n",
    "            [(0.05, 0.325), (0.275, 0.175)],\n",
    "            [(0.35, 0.75), (0.65, 0.75)],\n",
    "            [(0.95, 0.325), (0.725, 0.075)],\n",
    "        ]\n",
    "        \n",
    "        # Iterate over each influencer instrument, their respective arrows, and the color they're associated with\n",
    "        for influencer, start_coord, end_coord, col in zip(\n",
    "            instruments, start_coords, end_coords, self.colors\n",
    "        ):\n",
    "            # Iterate over each instrument they influence, and each individual arrow\n",
    "            for influenced, (x, y), (x2, y2) in zip(\n",
    "                [i for i in instruments if i != influencer], start_coord, end_coord\n",
    "            ):\n",
    "                # Get our coupling coefficient\n",
    "                subs = self._get_coupling_coefficient(influenced, influencer)\n",
    "                # Add in the arrow\n",
    "                self.ax.annotate(\n",
    "                    '', xy=(x, y), xycoords=self.ax.transAxes, \n",
    "                    xytext=(x2, y2), textcoords=self.ax.transAxes,\n",
    "                    arrowprops=dict(\n",
    "                        width=arrow_mod * subs, edgecolor=col, lw=1.5, facecolor=col, headwidth=20\n",
    "                    )\n",
    "                )\n",
    "                self._add_coupling_constant(subs, x, x2, y, y2)\n",
    "       \n",
    "    def _get_coupling_coefficient(\n",
    "        self, influenced: str, influencer: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Helper function to get the coupling coefficient between two instruments,\n",
    "        the influencer and influenced.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.df[\n",
    "            (self.df['influencer'] == influenced) & \n",
    "            (self.df['influenced'] == influencer)\n",
    "        ]['coupling'].iloc[0]\n",
    "    \n",
    "    \n",
    "    def _add_coupling_constant(\n",
    "        self, constant, x, x2, y, y2, mod: float = 0.03\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds coupling coefficient \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the default annotation position, the midpoint of our arrow\n",
    "        x_pos = (x + x2) / 2\n",
    "        y_pos = (y + y2) / 2\n",
    "        # Bottom of plot\n",
    "        if y_pos < 0.3:\n",
    "            y_pos += mod\n",
    "        # Top left of plot\n",
    "        elif x_pos < 0.5:\n",
    "            y_pos += mod\n",
    "            x_pos -= (mod * 1.1)\n",
    "        # Right of plot\n",
    "        elif x_pos > 0.5:\n",
    "            y_pos += mod\n",
    "            x_pos += (mod * 1.1)\n",
    "        # Add in the text using the x and y position\n",
    "        self.ax.text(\n",
    "            x_pos, y_pos, round(abs(constant), 2), ha='center', va='center', fontsize=14\n",
    "        )\n",
    "    \n",
    "    def _add_cover_image(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds in the album cover artwork to the plot \n",
    "        \"\"\"\n",
    "        \n",
    "        # Read the image and add it to the plot\n",
    "        img = plt.imread(r'/content/assets/cover.jpg')\n",
    "        img = mpl.offsetbox.OffsetImage(img, zoom=0.5)\n",
    "        ab = mpl.offsetbox.AnnotationBbox(\n",
    "            img, (0.5, 0.5), xycoords='data', \n",
    "            bboxprops=dict(edgecolor='black', boxstyle='sawtooth', lw=2)\n",
    "        )\n",
    "        self.ax.add_artist(ab)\n",
    "        # Add in the caption to the album artwork\n",
    "        txt = \"\"\"$Peri's$ $Scope$ (1959)\\nRhythmic adaptation\\nbetween musicians\"\"\"\n",
    "        self.ax.text(0.5, 0.325, txt, ha='center', va='center', fontsize=15)\n",
    "    \n",
    "    def _add_musicians_images(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds images corresponding to each performer in the trio\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate through the position for each picture, the amount of zoom,\n",
    "        # the name of the performer, and the colour of the box around the picture\n",
    "        for (x, y), zoom, txt, col  in zip(\n",
    "            [(0.125, 0.125), (0.5, 0.875), (0.867, 0.133),],\n",
    "            [.68, .6375, .75,],\n",
    "            ['Scott LaFaro', 'Paul Motian', 'Bill Evans',],\n",
    "            self.colors\n",
    "        ):\n",
    "            # Get the filepath from the performer's name\n",
    "            mus = txt.split(' ')[0].lower()\n",
    "            # Display the image\n",
    "            img = plt.imread(fr'/content/assets/{mus}.jpg')\n",
    "            img = mpl.offsetbox.OffsetImage(img, zoom=zoom)\n",
    "            ab = mpl.offsetbox.AnnotationBbox(\n",
    "                img, (x, y), xycoords='data', bboxprops=dict(edgecolor=col, lw=2)\n",
    "            )\n",
    "            self.ax.add_artist(ab)\n",
    "            # Add the text in, adjacent to the image\n",
    "            self.ax.text(\n",
    "                x, y + 0.15 if y < 0.5 else y - 0.15, \n",
    "                txt, ha='center', va='center', color=col\n",
    "            )\n",
    "            \n",
    "    def _add_extra_text(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Adds some additional extra text into the plot, e.g. titles, descriptions\n",
    "        \"\"\"\n",
    "        txt = \"\"\"Modelling interaction in the\\njazz rhythm section using \\nsource-seperated commercial\\naudio recordings\"\"\"\n",
    "        self.ax.text(-0.05, 0.9, txt, fontsize=12, ha='left', va='center', fontstyle='italic')\n",
    "        txt = \"\"\"Proof of Concept\"\"\"\n",
    "        self.ax.text(-0.05, 0.8, txt, fontsize=12, ha='left', va='center', fontweight='bold')\n",
    "        txt = \"\"\"Direction of arrows indicate the\\ntendency of one performer\\nto follow (adapt to) another;\\nthickness and colour of the\\narrows indicate coupling\\nstrength.\"\"\"\n",
    "        self.ax.text(1.05, 0.875, txt, fontsize=12, ha='right', va='center',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad444e80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "ad444e80",
    "outputId": "922ee8c0-5118-4025-b5d7-edf464423ba6"
   },
   "outputs": [],
   "source": [
    "vis = PhaseCorrectionPlot(df=df_coefs)\n",
    "vis.create_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c252e",
   "metadata": {
    "id": "fd6c252e"
   },
   "source": [
    "Even from the small, 30-second excerpt we've used here, the graph above shows some interesting details. Namely, both the bass and drums are tightly coupled to each other, but not to the piano soloist. The piano, meanwhile, couples strongly to both the bass and drums. In this sense, rhythmic adaptation is orientated towards the bass and drums comping, which provides an anchor for the piano improvisations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
